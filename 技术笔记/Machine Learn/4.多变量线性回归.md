# 多变量线性回归 #
<u>*由0到1难，由1到n还是难*</u>
## 多维特性 (Multiple features) ##
n个特征/输入，有假设`hypothesis`如下
$$ h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \cdots + \theta_{m}x_{n} $$

当 \(x_{0} = 1\)时，有

$$ h_{\theta}(x) = \theta_{0}x_{0} + \theta_{1}x_{1} + \cdots + \theta_{m}x_{n} =  \theta^{T}X $$

## 多变量梯度下降 ##

- Hypthesis: \(h_{\theta}(x) = \theta_{0}x_{0} + \theta_{1}x_{1} + \cdots + \theta_{m}x_{n}\)
- Parameters: \(\theta_{0},\theta_{1},\cdots,\theta_{n}\)
- Cost function : \(J(\theta_{0},\theta_{1},\cdots,\theta_{n}) = \frac{1}{2m} \sum_{}^{m}\left ( h_{\theta}(x^{\left ( i \right )}) - y^{\left ( i \right )} \right ) ^{2}\)
- Gradient descent: \(\theta_{j}:=\theta_{j} - \alpha\frac{\partial }{\partial \theta_{j}}J(\theta_{0},\theta_{1},\cdots,\theta_{n})\)

## 特征缩放 (Feature Scalling) ##

>一般的，执行特征缩放时，将特征的取值约束在-1到+1的范围内。

在特征缩放过程中，除了将特征除以最大值意外，也可以进行一个称为均值归一化的工作`mean normalization`。如果有一个特征\(x_{i}\)，可以用\(x_{i} - u_{i}\) 进行代替，\(u_{i}\) 是所有\(x_{i}\) 的平均值， 这样具有为0的平均值。

$$ \left ( x_{i} - u_{i} \right) / S_{i} = \left ( x_{i} - u_{i} \right) / (x^i_{max} - x^i_{min}) $$

## 学习速率 （Learning rate） ##
