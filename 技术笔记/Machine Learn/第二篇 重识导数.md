# 重识导数 #

<font color =red><u>*蓦然回首，发现全已不懂*</u></font>

导数是什么？
>一个函数在某一点附近的变化率，也就是微小变化的dx,引起多大的dy。

对于一个隐函数 \(F(X)(X=\{x_{0},x_{1},\cdots ,x_{n}\})\)，其在`X`的各个方向上变化速率和方向都不相同。怎样表述给定`X`的情况下，`X`附近`F(X)`的变化率和速率？

>梯度是`F(X)`对各个分量求偏导后的结果，代表了`F(X)`在各个方向的变化率。整个法向量就是`F(X)`在各个方向上变化率叠加出来的向量。可以在使用全微分公式后，用`i`,`j`,`k`...代替`dx1`，`dx2`，`dx3`...来表示法向量。

为什么常说函数取极值的时候倒数为`0` ？

> 一维时有\(F(X) = x^2\)求极小值，微分后的\(dF(x) = 2xdx\)，当\(x = 0\)时，导数\(2x\)为0，取得极值。

> 二维时\(dF(x,y) = F_{x}(x,y)dx + F_{y}(x,y)dy=\begin{bmatrix}F_{x}(x,y)&F_{y}(x,y)\end{bmatrix}\begin{bmatrix}dx \\ dy \\ \end{bmatrix}\)，\(F_{x}(x,y)\) 和\(F_{y}(x,y)\) 的值在计算后会有正负值，通过调整\(dx,dy\)的正负号，也就是确定怎么移动\(x\)和\(y\)，就可以使\(F(x,y)\) 的值变大变小。只有在偏导数全部为`0`的情况下，无论怎样调整 \(dx\) 和 \(dy\) \(F(x,y)\)都是`0`;

为什么计算函数极值时还要用梯度下降法而不直接令导数为`0`来求解函数极值？

> a. 并不是所有函数都可以求出导数为`0`的点. </br>
> b. 可以求出导数在每个点的值，但是不能直接求解方程。</br>
> c. 导数并没有解析解，像一个黑匣子一样，给定输入值，可以返回输出值。但是根本无法获知里面具体什么情况。神经网络更是如此。

牛顿迭代和梯度下降法求极值的区别？

> 梯度下降法的算法复杂度低一些，但是迭代次数多一些。牛顿迭代法计算收敛的更快一些，但是由于存在矩阵求逆所以每一步的计算量很大。
