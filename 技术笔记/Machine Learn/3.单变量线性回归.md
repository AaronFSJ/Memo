# 单变量线性回归 (Linear Regression with One variable) #
`hypothesis`

## 模型表达 ##
常用的符号定义：
- \(m\) 代表训练集中实例的数量
- \(x\) 代表特征\输入变量
- \(y\) 代表目标变量\输出变量
- \((x,y)\) 代表训练集中的实例
- \((x^{\left ( i \right )}, y^{\left ( i \right )})\) 代表第i个观察实例
- \(h\) 代表学习算法的解决方案或者函数，也称为假设 (`hypothesis`)

一个训练数据集`Training Set`通过学习算法`Learning Algorithm`训练假设`hypothesis`，当有输入通过`hypothesis`，会有相应的符合训练集特征的输出。当假设`hypothesis`只含有一个特征/输入变量时，叫做单变量线性回归问题。
$$ h_{\theta } = \theta_{0} + \theta_{1}x $$

## 代价函数 (Cost function) ##

\(\theta_{0}\) 和 \(\theta_{1}\) 是模型参数。 我们要做的就是确定模型参数，使通过训练数据集，使得输入\(x\) 时，得出的结果 \(h_{\theta}(x^i)\) 最接近数据集给出的真实的\(y\) 。这里涉及到计算值与真实值误差的最小化问题。换一个角度就是使得建模误差的平方和能够最小的模型参数即是代价函数
$$ J(\theta_{0},\theta_{1}) = \frac{1}{2m}\sum_{}^{m}\left ( h_{\theta}(x^{\left ( i \right )}) - y^{\left ( i \right )} \right ) ^{2} $$

## 梯度下降 (Gradient Descent) ##
梯度下降是很常用的算法，通过它可以将代价函数最小化，它不仅被用在线性回归上，也被广泛应用于机器学习的众多领域中。

假设有一个代价函数 \(J \left ( \theta_{0},\theta_{1},\cdots ,\theta_{n} \right )\)，我们可以通过不停的一点点改变 \({\theta_{i}}\) 找到\(J\)的最小值或者局部最小值。其特点是不同的起始点开始进行梯度下降算法，可能会得到一个非常不同的局部最优解。
